\chapter{Abstract}
In the real world, there are several optimization problems in engineering, whose input-output relationships are too complex and indistinct. These problems are usually bounded but the underlying function and constraints acting on them cannot be determined. Owing to computational expensiveness of these problems, the algorithm is designed to use minimum function evaluations. This work presents an approach to solving the black-box optimization problem using Response Surface Methodology (RSM) using ALAMO, an algebraic modeling framework for modeling precise but low complexity surrogate functions in order to optimize with least number of function evaluations. To make the algorithm adaptable for high dimensions of feature space, the RSM is applied in multi-coordinate search manner. The uniqueness of the approach lies in applying the Machine Learning concept of Nesterov Momentum that boosts the convergence of the problems. This algorithm's convergence was usually third or fourth highest amongst all the compared categories and the best for convex non-smooth problems. A case-study on a hidden constrained problem showed convergence to the local optima.

\noindent
\textbf{Keywords:} Black-box optimization, Derivative free, Global optimization, Accelerated gradient, Non-smooth, Non-convex
